# rag_updater_fixed.py
import asyncio
import redis.asyncio as redis
import json
import logging
import asyncpg
from datetime import datetime
import os
from zhipuai import ZhipuAI

# --- é…ç½® (ä¸æ‚¨åŸç‰ˆç›¸åŒ) ---
# ZHIPUAI_API_KEY = os.getenv("ZHIPUAI_API_KEY", "ca13e6189e2d4fb7b81946c5d77b9219.fNMK7R94xrqvdxzo")
ZHIPUAI_API_KEY = "ca13e6189e2d4fb7b81946c5d77b9219.fNMK7R94xrqvdxzo"

DB_USER = 'root'
DB_PASS = '123456'
DB_NAME = 'chat_db'
DB_HOST = 'localhost'
DB_PORT = 5432
REDIS_HOST = 'localhost'
REDIS_PORT = 6379
REDIS_DB = 1
# --- é…ç½®ç»“æŸ ---

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s')

# --- å…¨å±€å˜é‡ ---
db_pool = None
llm_client = None
# âœ¨ æ–°å¢ï¼šç”¨äºè·Ÿè¸ªæ‰€æœ‰æ­£åœ¨è¿è¡Œçš„åå°ä»»åŠ¡
active_tasks = set()
# âœ¨ æ–°å¢ï¼šç”¨äºä¼˜é›…å…³é—­çš„äº‹ä»¶
shutdown_event = asyncio.Event()


async def init_services():
    """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± å’ŒLLMå®¢æˆ·ç«¯ (ä¸æ‚¨åŸç‰ˆåŸºæœ¬ç›¸åŒ)"""
    global db_pool, llm_client
    logging.info("ğŸš€ å¼€å§‹åˆå§‹åŒ–æœåŠ¡...")
    try:
        logging.info("ğŸ“Š æ­£åœ¨è¿æ¥PostgreSQLæ•°æ®åº“...")
        db_pool = await asyncio.wait_for(
            asyncpg.create_pool(user=DB_USER, password=DB_PASS, database=DB_NAME, host=DB_HOST, port=DB_PORT, min_size=1, max_size=10),
            timeout=10.0
        )
        async with db_pool.acquire() as conn:
            await conn.execute('''
                CREATE TABLE IF NOT EXISTS interactions (
                    id SERIAL PRIMARY KEY, type TEXT NOT NULL, session_id TEXT NOT NULL,
                    device_id TEXT NOT NULL, text TEXT, timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    speaker TEXT
                )''')
            logging.info("ğŸ’¿ è¡¨ 'interactions' å·²å‡†å¤‡å°±ç»ª.")
            await conn.execute('''
                CREATE TABLE IF NOT EXISTS user_profiles (
                    id SERIAL PRIMARY KEY, device_id TEXT NOT NULL UNIQUE,
                    profile_data JSONB, last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )''')
            logging.info("ğŸ’¿ è¡¨ 'user_profiles' å·²å‡†å¤‡å°±ç»ª.")
        logging.info("âœ… PostgreSQLæ•°æ®åº“è¿æ¥æˆåŠŸï¼")
    except Exception as e:
        logging.error(f"âŒ åˆå§‹åŒ–æ•°æ®åº“å¤±è´¥: {e}")
        raise

    logging.info("ğŸ¤– æ­£åœ¨åˆå§‹åŒ–LLMå®¢æˆ·ç«¯...")
    if not ZHIPUAI_API_KEY:
        logging.warning("âš ï¸ æœªé…ç½®æœ‰æ•ˆçš„ ZHIPUAI_API_KEYï¼Œç”¨æˆ·ç”»åƒæ€»ç»“åŠŸèƒ½å°†æ— æ³•ä½¿ç”¨ã€‚")
    else:
        llm_client = ZhipuAI(api_key=ZHIPUAI_API_KEY)
        logging.info("ğŸ¤– LLM å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸã€‚")
    logging.info("âœ… æ‰€æœ‰æœåŠ¡åˆå§‹åŒ–å®Œæˆï¼")

# --- LLM æ€»ç»“éƒ¨åˆ† (ä¸æ‚¨åŸç‰ˆç›¸åŒ) ---
async def summarize_conversation_with_llm(conversation_text, existing_profile):
    """ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹æ€»ç»“å¯¹è¯å¹¶æ›´æ–°ç”¨æˆ·ç”»åƒ"""
    if not llm_client:
        logging.warning("LLM å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æ€»ç»“ã€‚")
        return existing_profile or {}
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„ç”¨æˆ·è¡Œä¸ºåˆ†æå¸ˆã€‚è¯·æ ¹æ®ä»¥ä¸‹ç”¨æˆ·ä¸AIçš„å¯¹è¯è®°å½•ï¼Œæ€»ç»“å¹¶æ›´æ–°è¯¥ç”¨æˆ·çš„ä¸ªäººç”»åƒã€‚
ç”»åƒåº”ä»¥JSONæ ¼å¼è¾“å‡ºï¼ŒåŒ…å«ä½†ä¸é™äºä»¥ä¸‹å­—æ®µï¼š
- interests (å…´è¶£çˆ±å¥½), personality (æ€§æ ¼ç‰¹ç‚¹), known_facts (å·²çŸ¥ä¿¡æ¯), potential_needs (æ½œåœ¨éœ€æ±‚)
è¯·åœ¨å·²æœ‰ç”»åƒçš„åŸºç¡€ä¸Šè¿›è¡Œè¡¥å……å’Œæ›´æ–°ï¼Œè€Œä¸æ˜¯å®Œå…¨æ›¿æ¢ã€‚
[å·²æœ‰ç”»åƒ]
{json.dumps(existing_profile, indent=2, ensure_ascii=False)}
[æœ¬æ¬¡å¯¹è¯è®°å½•]
{conversation_text}
[ä½ çš„è¾“å‡º (è¯·åªè¿”å›JSONå¯¹è±¡)]
"""
    try:
        logging.info(f"æ­£åœ¨ä¸ºä»¥ä¸‹å¯¹è¯è°ƒç”¨ LLMï¼š\n---\n{conversation_text[:200]}...\n---")
        response = await asyncio.to_thread(
            llm_client.chat.completions.create,
            model="glm-4-flash-250414",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        summary_json_str = response.choices[0].message.content
        new_profile = json.loads(summary_json_str)
        logging.info(f"âœ… LLM æ€»ç»“å®Œæˆï¼Œç”Ÿæˆæ–°ç”»åƒ: {new_profile}")
        return new_profile
    except Exception as e:
        logging.error(f"âŒ è°ƒç”¨ LLM å¤±è´¥: {e}", exc_info=True) # exc_info=True ä¼šæ‰“å°å®Œæ•´çš„é”™è¯¯å †æ ˆ
        return existing_profile


async def process_session_summary(session_id, device_id):
    """å¤„ç†å•ä¸ªä¼šè¯çš„æ€»ç»“ä»»åŠ¡"""
    # ğŸ’¡ æ”¹è¿›ï¼šå»æ‰äº†ä¸å¯é çš„ sleep(0.5)ã€‚
    # å‡è®¾ session_end æ¶ˆæ¯æ€»æ˜¯åœ¨å¯¹è¯æ¶ˆæ¯ä¹‹ååˆ°è¾¾ã€‚
    logging.info(f"å¼€å§‹å¤„ç†ä¼šè¯æ€»ç»“: session_id={session_id}, device_id={device_id}")
    try:
        async with db_pool.acquire() as conn:
            records = await conn.fetch("SELECT type, text, speaker FROM interactions WHERE session_id = $1 ORDER BY timestamp ASC", session_id)
            if not records:
                logging.warning(f"æœªæ‰¾åˆ°ä¼šè¯ {session_id} çš„ä»»ä½•è®°å½•ï¼Œæ— æ³•æ€»ç»“ã€‚")
                return

            conversation_text = "\n".join(f"{rec['speaker'] or rec['type']}: {rec['text']}" for rec in records)
            profile_record = await conn.fetchrow("SELECT profile_data FROM user_profiles WHERE device_id = $1", device_id)
            existing_profile = profile_record['profile_data'] if profile_record else {}
            
            logging.info(f"è·å–åˆ°è®¾å¤‡ {device_id} çš„ç°æœ‰ç”»åƒ: {existing_profile}")
            new_profile = await summarize_conversation_with_llm(conversation_text, existing_profile)

            # åªæœ‰åœ¨ç”»åƒç¡®å®æ›´æ–°äº†çš„æƒ…å†µä¸‹æ‰å†™å…¥æ•°æ®åº“
            if new_profile and new_profile != existing_profile:
                await conn.execute(
                    """
                    INSERT INTO user_profiles (device_id, profile_data, last_updated) VALUES ($1, $2, NOW())
                    ON CONFLICT (device_id) DO UPDATE SET profile_data = $2, last_updated = NOW();
                    """,
                    device_id, json.dumps(new_profile)
                )
                logging.info(f"âœ… æˆåŠŸæ›´æ–°è®¾å¤‡ {device_id} çš„ç”¨æˆ·ç”»åƒã€‚")
            else:
                logging.info(f"ç”»åƒæ— å˜åŒ–ï¼Œæœªæ›´æ–°æ•°æ®åº“ã€‚")

    except Exception as e:
        # ç¡®ä¿å³ä½¿åœ¨è¿™ä¸ªä»»åŠ¡å†…éƒ¨å‡ºé”™ï¼Œæˆ‘ä»¬ä¹Ÿèƒ½çœ‹åˆ°æ—¥å¿—
        logging.error(f"âŒ å¤„ç†ä¼šè¯æ€»ç»“ {session_id} æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)


async def save_interaction(data):
    """å°†å•æ¡äº¤äº’æ¶ˆæ¯å­˜å…¥æ•°æ®åº“"""
    try:
        async with db_pool.acquire() as conn:
            timestamp = data.get('timestamp')
            if isinstance(timestamp, (int, float)):
                timestamp = datetime.fromtimestamp(timestamp)
            
            await conn.execute(
                """
                INSERT INTO interactions (type, session_id, device_id, text, speaker, timestamp)
                VALUES ($1, $2, $3, $4, $5, $6)
                """,
                data.get('type'), data.get('session_id'), data.get('device_id'),
                data.get('text'), data.get('speaker'), timestamp
            )
            logging.info(f"ğŸ’¾ å·²ä¿å­˜æ¶ˆæ¯åˆ°æ•°æ®åº“: session={data.get('session_id')}, type={data.get('type')}")
    except Exception as e:
        logging.error(f"âŒ ä¿å­˜äº¤äº’ä¿¡æ¯åˆ°æ•°æ®åº“å¤±è´¥: {e}", exc_info=True)


def create_safe_task(coro):
    """
    âœ¨ æ–°å¢ï¼šä¸€ä¸ªå®‰å…¨çš„åˆ›å»ºä»»åŠ¡çš„å‡½æ•°ã€‚
    å®ƒä¼šåˆ›å»ºä»»åŠ¡ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°æˆ‘ä»¬çš„å…¨å±€ä»»åŠ¡é›†åˆä¸­ï¼Œ
    å¹¶è®¾ç½®ä¸€ä¸ªâ€œå®Œæˆå›è°ƒâ€æ¥å¤„ç†å¼‚å¸¸å’Œæ¸…ç†ã€‚
    """
    task = asyncio.create_task(coro)
    active_tasks.add(task)
    # å½“ä»»åŠ¡å®Œæˆæ—¶ï¼ˆæ— è®ºæˆåŠŸã€å¤±è´¥è¿˜æ˜¯å–æ¶ˆï¼‰ï¼Œè¿™ä¸ªå›è°ƒå‡½æ•°éƒ½ä¼šè¢«è°ƒç”¨
    task.add_done_callback(task_done_callback)
    return task

def task_done_callback(task):
    """âœ¨ æ–°å¢ï¼šä»»åŠ¡å®Œæˆæ—¶çš„å›è°ƒå‡½æ•°"""
    try:
        # å¦‚æœä»»åŠ¡åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°äº†å¼‚å¸¸ï¼Œè°ƒç”¨ .result() ä¼šé‡æ–°æŠ›å‡ºè¯¥å¼‚å¸¸
        task.result()
    except asyncio.CancelledError:
        pass  # ä»»åŠ¡è¢«å–æ¶ˆæ˜¯æ­£å¸¸æƒ…å†µï¼Œä¸ç”¨è®°å½•é”™è¯¯
    except Exception as e:
        # ğŸ’¥ è¿™é‡Œæ˜¯å…³é”®ï¼æ•è·å¹¶è®°å½•æ‰€æœ‰â€œè¢«é—å¿˜çš„â€ä»»åŠ¡ä¸­çš„å¼‚å¸¸
        logging.error(f"åå°ä»»åŠ¡å‘ç”Ÿæœªæ•è·çš„å¼‚å¸¸: {e}", exc_info=True)
    finally:
        # æ— è®ºå¦‚ä½•ï¼Œéƒ½å°†ä»»åŠ¡ä»æ´»åŠ¨é›†åˆä¸­ç§»é™¤
        active_tasks.remove(task)


async def message_consumer(queue):
    """
    âœ¨ æ–°å¢ï¼šæ¶ˆæ¯æ¶ˆè´¹è€…ã€‚
    å®ƒä»é˜Ÿåˆ—ä¸­è·å–æ¶ˆæ¯å¹¶è¿›è¡Œå¤„ç†ï¼Œå°†é€»è¾‘è§£è€¦ã€‚
    """
    while not (shutdown_event.is_set() and queue.empty()):
        try:
            message_data = await asyncio.wait_for(queue.get(), timeout=1.0)
            data = json.loads(message_data)
            
            if data.get('type') == 'session_end':
                logging.info(f"æ£€æµ‹åˆ°ä¼šè¯ç»“æŸä¿¡å·: {data.get('session_id')}ï¼Œå‡†å¤‡è§¦å‘æ€»ç»“ã€‚")
                create_safe_task(process_session_summary(data.get('session_id'), data.get('device_id')))
            else:
                await save_interaction(data)
            queue.task_done()
        except asyncio.TimeoutError:
            continue # é˜Ÿåˆ—ä¸ºç©ºï¼Œç»§ç»­ç­‰å¾…
        except json.JSONDecodeError:
            logging.warning(f"æ— æ³•è§£ææ¶ˆæ¯: {message_data}")
            queue.task_done()
        except Exception as e:
            logging.error(f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {e}", exc_info=True)
            queue.task_done()


async def redis_producer(queue):
    """
    âœ¨ é‡æ„ï¼šç°åœ¨åªè´Ÿè´£ä»Redisæ¥æ”¶æ¶ˆæ¯å¹¶æ”¾å…¥å†…éƒ¨é˜Ÿåˆ—ã€‚
    """
    logging.info("ğŸ”„ å¼€å§‹è¿æ¥Redis...")
    while not shutdown_event.is_set():
        try:
            r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=REDIS_DB, decode_responses=True)
            async with r.pubsub() as pubsub:
                await pubsub.subscribe('xiaozhi_interactions')
                logging.info(f"å·²è®¢é˜…é¢‘é“ 'xiaozhi_interactions'ï¼Œåœ¨ {REDIS_HOST}:{REDIS_PORT} ç­‰å¾…æ¶ˆæ¯...")
                while not shutdown_event.is_set():
                    message = await pubsub.get_message(ignore_subscribe_messages=True, timeout=1.0)
                    if message and message['type'] == 'message':
                        logging.info(f"ğŸ“¥ ä»Redisæ¥æ”¶åˆ°æ¶ˆæ¯ï¼Œæ”¾å…¥å¤„ç†é˜Ÿåˆ—...")
                        await queue.put(message['data'])
        except redis.exceptions.ConnectionError as e:
            if not shutdown_event.is_set():
                logging.error(f"Redis è¿æ¥å¤±è´¥: {e}. 5ç§’åé‡è¯•...")
                await asyncio.sleep(5)
        except Exception as e:
            if not shutdown_event.is_set():
                logging.error(f"Redisç”Ÿäº§è€…å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}. 5ç§’åé‡è¯•...", exc_info=True)
                await asyncio.sleep(5)



async def main():
    """ä¸»ç¨‹åºå…¥å£ (Windows å…¼å®¹ç‰ˆ)"""
    
    # åˆå§‹åŒ–æœåŠ¡
    try:
        await init_services()
    except Exception as e:
        logging.critical(f"æœåŠ¡åˆå§‹åŒ–å¤±è´¥ï¼Œç¨‹åºæ— æ³•å¯åŠ¨: {e}")
        return # å¦‚æœæœåŠ¡éƒ½èµ·ä¸æ¥ï¼Œç›´æ¥é€€å‡º

    # æ ¸å¿ƒæ”¹åŠ¨ï¼šä½¿ç”¨é˜Ÿåˆ—æ¥è§£è€¦ç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…
    message_queue = asyncio.Queue(maxsize=100)
    
    producer_task = asyncio.create_task(redis_producer(message_queue))
    consumer_task = asyncio.create_task(message_consumer(message_queue))

    logging.info("ğŸš€ æœåŠ¡å·²å¯åŠ¨ï¼ŒæŒ‰ CTRL+C é€€å‡ºã€‚")

    # âœ¨ æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨ try...except KeyboardInterrupt æ¥å¤„ç† Windows ä¸Šçš„ Ctrl+C
    try:
        # åœ¨ Linux/macOS ä¸Šï¼Œä»ç„¶å¯ä»¥ä½¿ç”¨ä¿¡å·
        # ä½†ä¸ºäº†è·¨å¹³å°ä¸€è‡´æ€§ï¼Œè¿™é‡Œæˆ‘ä»¬ç»Ÿä¸€ä½¿ç”¨æ›´ç®€å•çš„æ–¹å¼
        # æŒç»­è¿è¡Œï¼Œç›´åˆ° producer æˆ– consumer æ„å¤–é€€å‡º
        await asyncio.gather(producer_task, consumer_task)

    except KeyboardInterrupt:
        logging.info("æ”¶åˆ° CTRL+C ä¿¡å·ï¼æ­£åœ¨å‡†å¤‡ä¼˜é›…é€€å‡º...")
    except asyncio.CancelledError:
        # å¦‚æœä»»åŠ¡è¢«å¤–éƒ¨å–æ¶ˆï¼Œä¹Ÿè§†ä¸ºæ­£å¸¸é€€å‡º
        logging.info("ä¸»ä»»åŠ¡è¢«å–æ¶ˆï¼Œå¼€å§‹å…³é—­æµç¨‹...")
    
    finally:
        # è§¦å‘å…³é—­äº‹ä»¶ï¼Œé€šçŸ¥æ‰€æœ‰å¾ªç¯ä»»åŠ¡åœæ­¢
        shutdown_event.set()

        logging.info("æ­£åœ¨å…³é—­ç¨‹åº...")
        
        # 1. ç»™ç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…ä¸€ç‚¹æ—¶é—´æ¥å“åº” shutdown_event
        # gather å®ƒä»¬ï¼Œè®©å®ƒä»¬æœ‰æœºä¼šå¹²å‡€åœ°é€€å‡ºå¾ªç¯
        await asyncio.gather(producer_task, consumer_task, return_exceptions=True)
        logging.info("ç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…ä»»åŠ¡å·²åœæ­¢ã€‚")
        
        # 2. ç­‰å¾…é˜Ÿåˆ—ä¸­çš„ç°æœ‰æ¶ˆæ¯è¢«å¤„ç†å®Œæ¯•
        # å› ä¸ºæ¶ˆè´¹è€…å¯èƒ½å·²ç»åœæ­¢ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ–°çš„å°ä»»åŠ¡æ¥æ¸…ç©ºé˜Ÿåˆ—
        final_consumer = asyncio.create_task(message_consumer(message_queue))
        await message_queue.join()
        final_consumer.cancel()
        await asyncio.gather(final_consumer, return_exceptions=True)
        logging.info("æ¶ˆæ¯é˜Ÿåˆ—å·²æ¸…ç©ºã€‚")

        # 3. ç­‰å¾…æ‰€æœ‰åå°ä»»åŠ¡ï¼ˆå¦‚LLMè°ƒç”¨ï¼‰å®Œæˆ
        if active_tasks:
            logging.info(f"ç­‰å¾… {len(active_tasks)} ä¸ªåå°ä»»åŠ¡å®Œæˆ...")
            # è®¾ç½®ä¸€ä¸ªè¶…æ—¶ï¼Œé˜²æ­¢ä»»åŠ¡æ°¸è¿œå¡ä½
            done, pending = await asyncio.wait(active_tasks, timeout=30.0)
            
            if pending:
                logging.warning(f"{len(pending)} ä¸ªåå°ä»»åŠ¡è¶…æ—¶ï¼Œå°†è¢«å¼ºåˆ¶å–æ¶ˆã€‚")
                for task in pending:
                    task.cancel()
                await asyncio.gather(*pending, return_exceptions=True)

            logging.info("æ‰€æœ‰åå°ä»»åŠ¡å·²å¤„ç†å®Œæ¯•ã€‚")

        # 4. å…³é—­æ•°æ®åº“è¿æ¥æ± 
        if db_pool:
            await db_pool.close()
            logging.info("æ•°æ®åº“è¿æ¥æ± å·²å…³é—­ã€‚")
            
        logging.info("ğŸ‘‹ ç¨‹åºå·²ä¼˜é›…é€€å‡ºã€‚")


if __name__ == "__main__":
    # âœ¨ ä¿®æ”¹ï¼šè¿™é‡Œä¹Ÿç”¨ try/except åŒ…è£¹ä¸€ä¸‹ï¼Œå¯ä»¥æ•è·å¯åŠ¨è¿‡ç¨‹ä¸­çš„ Ctrl+C
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logging.info("ç¨‹åºåœ¨å¯åŠ¨æ—¶è¢«å¼ºåˆ¶é€€å‡ºã€‚")